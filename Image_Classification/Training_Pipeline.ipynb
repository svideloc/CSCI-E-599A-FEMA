{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task:  Fine tune ConvNeXt V2 (facebook/convnextv2-huge-384)\n",
        "\n",
        "Architecture: ConvNeXt V2 builds on the success of ConvNeXt V1, which was designed to improve the efficiency and performance of convolutional networks, making them competitive with transformer models.\n",
        "\n",
        "Disaster imagery can include intricate details (e.g., damaged buildings, roads, etc.), and ConvNeXt V2 is particularly good at capturing such local patterns due to its advanced convolutional layers.\n",
        "State-of-the-art: ConvNeXt V2 is one of the most powerful convolutional models, with an architecture designed to handle large-scale image classification tasks like ImageNet. It has shown excellent performance in both high-level and fine-grained image tasks.\n",
        "Efficiency: While it's large, ConvNeXt V2 is optimized for efficiency compared to some transformer models, making it more manageable in terms of computational cost for training."
      ],
      "metadata": {
        "id": "SIwGnczDuv3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "k_oVTJRRmzeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmgtDxMLmmPL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LADI dataset\n",
        "ds = load_dataset(\"MITLL/LADI-v2-dataset\", streaming=True)"
      ],
      "metadata": {
        "id": "x58y8JPCm-i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label keys for multi-label classification\n",
        "label_keys = ['bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater',\n",
        "              'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage',\n",
        "              'trees_any', 'trees_damage', 'water_any']"
      ],
      "metadata": {
        "id": "RQmoURxEnFOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and processor setup\n",
        "model_name = \"facebook/convnextv2-huge-22k-384\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Load the model while ignoring the size mismatch for the classifier layer\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_keys),\n",
        "    ignore_mismatched_sizes=True  # Ignore classifier weight size mismatch\n",
        ")"
      ],
      "metadata": {
        "id": "r2fMfVRvnKTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "muLwlWY_nOd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and gradient scaler for mixed precision\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "jVeKRLZ4oCZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image preprocessing transformation\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),  # Resize to match the input size of ConvNeXtV2\n",
        "    transforms.ToTensor(),  # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
        "])"
      ],
      "metadata": {
        "id": "-yPLlVjdoJLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# StreamDataset class for handling image and label processing\n",
        "class StreamDataset(IterableDataset):\n",
        "    def __init__(self, dataset, split_name, label_keys, image_transforms):\n",
        "        self.dataset = dataset\n",
        "        self.split_name = split_name\n",
        "        self.label_keys = label_keys\n",
        "        self.image_transforms = image_transforms\n",
        "\n",
        "    def process_item(self, item):\n",
        "        image = item['image']\n",
        "        labels = [int(item[key]) for key in self.label_keys]\n",
        "\n",
        "        # Apply transformations to the image\n",
        "        processed_image = self.image_transforms(image)\n",
        "        return processed_image, labels\n",
        "\n",
        "    def __iter__(self):\n",
        "        for item in self.dataset[self.split_name]:\n",
        "            yield self.process_item(item)\n",
        "\n",
        "# Function to process the dataset for training\n",
        "def process_dataset(model, dataset, split_name, label_keys, image_transforms, optimizer=None, train=False, batch_size=8):\n",
        "    model.train() if train else model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    processed_dataset = StreamDataset(dataset, split_name, label_keys, image_transforms)\n",
        "    loader = DataLoader(processed_dataset, batch_size=batch_size, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "    if not train:\n",
        "        torch.no_grad()\n",
        "\n",
        "    for batch_images, batch_labels in tqdm(loader):\n",
        "        batch_images = torch.stack(batch_images).to(device)\n",
        "        batch_labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "        if train:\n",
        "            with autocast():\n",
        "                outputs = model(batch_images)\n",
        "                loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, batch_labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(batch_images)\n",
        "                loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, batch_labels)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        logits = outputs.logits.cpu().detach().numpy()\n",
        "        predictions = torch.sigmoid(torch.tensor(logits)).cpu().detach().numpy()\n",
        "\n",
        "        all_preds.extend(predictions)\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return running_loss / len(all_preds), all_labels, all_preds\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "batch_size = 8\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training step\n",
        "    train_loss, train_labels, train_preds = process_dataset(model, ds, 'train', label_keys, image_transforms, optimizer, train=True, batch_size=batch_size)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    del train_labels, train_preds\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Validation step\n",
        "    val_loss, val_labels, val_preds = process_dataset(model, ds, 'validation', label_keys, image_transforms, batch_size=batch_size)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    del val_labels, val_preds\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Training complete. You can now evaluate the model using the evaluation pipeline.\")"
      ],
      "metadata": {
        "id": "HtH3CUbBoNnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2Mr7dUvtmuK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}