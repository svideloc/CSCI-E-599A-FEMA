{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "uJIwxhFhZupF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "cRlB9_bVZmFA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset in streaming mode\n",
        "ds = load_dataset(\"MITLL/LADI-v2-dataset\", streaming=True)"
      ],
      "metadata": {
        "id": "5t7vDwPYZ9M_",
        "outputId": "1685eca0-16f5-4d5e-da85-80120f3eafdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de5e837d33aa4aeab2d8a1d01673ec48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da35ac1ceba145c6adae037d20f2b46a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3165b25251e43249eaace272e79ecfd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)"
      ],
      "metadata": {
        "id": "Q2-qxwtWbC-i",
        "outputId": "fd26a7f4-d1b7-4765-9170-96098c74194a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IterableDatasetDict({\n",
            "    train: IterableDataset({\n",
            "        features: ['image', 'bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater', 'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage', 'trees_any', 'trees_damage', 'water_any'],\n",
            "        n_shards: 40\n",
            "    })\n",
            "    validation: IterableDataset({\n",
            "        features: ['image', 'bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater', 'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage', 'trees_any', 'trees_damage', 'water_any'],\n",
            "        n_shards: 40\n",
            "    })\n",
            "    test: IterableDataset({\n",
            "        features: ['image', 'bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater', 'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage', 'trees_any', 'trees_damage', 'water_any'],\n",
            "        n_shards: 40\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_example = next(iter(ds['train']))\n",
        "print(first_example.keys())"
      ],
      "metadata": {
        "id": "7f1I0DP3bJWv",
        "outputId": "5320d282-0784-4e06-a75b-53e1cb9dce6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater', 'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage', 'trees_any', 'trees_damage', 'water_any'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Labels' in first_example:\n",
        "  print('Label Exist!')\n",
        "  print(f\" First example label: {first_example['Labels']}\")\n",
        "else:\n",
        "  print('Label does not exist in this dataset')"
      ],
      "metadata": {
        "id": "_D-JuWT7dQVA",
        "outputId": "5849c25a-8749-40cd-d264-88a34786d39d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label does not exist in this dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_example"
      ],
      "metadata": {
        "id": "8bP1kHkAd97I",
        "outputId": "7c676241-2fe8-428e-bd6e-742ef9b55de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1200>,\n",
              " 'bridges_any': False,\n",
              " 'buildings_any': False,\n",
              " 'buildings_affected_or_greater': False,\n",
              " 'buildings_minor_or_greater': False,\n",
              " 'debris_any': False,\n",
              " 'flooding_any': False,\n",
              " 'flooding_structures': False,\n",
              " 'roads_any': False,\n",
              " 'roads_damage': False,\n",
              " 'trees_any': True,\n",
              " 'trees_damage': True,\n",
              " 'water_any': True}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List out possible labels\n",
        "possible_labels = [key for key, value in first_example.items() if key != 'image']\n",
        "\n",
        "print(f\"Possible labels: {possible_labels}\")"
      ],
      "metadata": {
        "id": "Gq4byqAWekhd",
        "outputId": "d6e1cbf4-ab80-495f-a97d-ba9d17d3af27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible labels: ['bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater', 'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage', 'trees_any', 'trees_damage', 'water_any']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset\n",
        "for i, example in enumerate(ds['train']):  # Replace 'train' with the correct split\n",
        "    image = example['image']\n",
        "\n",
        "    # Check if 'label' exists or access another field\n",
        "    label = example.get('label', None)  # Safely get 'label', or None if it doesn't exist\n",
        "\n",
        "    # If no direct 'label', check other features\n",
        "    if label is None:\n",
        "        # Multi-label scenario: list of possible binary labels\n",
        "        label = {key: value for key, value in example.items() if key != 'image'}\n",
        "\n",
        "    # Print image and label\n",
        "    print(f\"Image {i}: {image}\")\n",
        "    print(f\"Label {i}: {label}\")\n",
        "\n",
        "    if i >= 5:  # Stop after 5 examples\n",
        "        break"
      ],
      "metadata": {
        "id": "qv90eolgf5sz",
        "outputId": "cc577130-93f2-4ef8-c05d-15a5f24a1a14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 0: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1200 at 0x79C9C958E950>\n",
            "Label 0: {'bridges_any': False, 'buildings_any': False, 'buildings_affected_or_greater': False, 'buildings_minor_or_greater': False, 'debris_any': False, 'flooding_any': False, 'flooding_structures': False, 'roads_any': False, 'roads_damage': False, 'trees_any': True, 'trees_damage': True, 'water_any': True}\n",
            "Image 1: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1200 at 0x79C9C958E7D0>\n",
            "Label 1: {'bridges_any': False, 'buildings_any': True, 'buildings_affected_or_greater': False, 'buildings_minor_or_greater': False, 'debris_any': False, 'flooding_any': False, 'flooding_structures': False, 'roads_any': True, 'roads_damage': False, 'trees_any': True, 'trees_damage': False, 'water_any': False}\n",
            "Image 2: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1196 at 0x79C9C958F1C0>\n",
            "Label 2: {'bridges_any': False, 'buildings_any': False, 'buildings_affected_or_greater': False, 'buildings_minor_or_greater': False, 'debris_any': False, 'flooding_any': False, 'flooding_structures': False, 'roads_any': True, 'roads_damage': False, 'trees_any': True, 'trees_damage': False, 'water_any': False}\n",
            "Image 3: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1200 at 0x79C9C958F640>\n",
            "Label 3: {'bridges_any': False, 'buildings_any': True, 'buildings_affected_or_greater': True, 'buildings_minor_or_greater': False, 'debris_any': True, 'flooding_any': False, 'flooding_structures': False, 'roads_any': True, 'roads_damage': False, 'trees_any': True, 'trees_damage': True, 'water_any': False}\n",
            "Image 4: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1200 at 0x79C9C958FD90>\n",
            "Label 4: {'bridges_any': False, 'buildings_any': True, 'buildings_affected_or_greater': False, 'buildings_minor_or_greater': False, 'debris_any': False, 'flooding_any': False, 'flooding_structures': False, 'roads_any': True, 'roads_damage': False, 'trees_any': True, 'trees_damage': False, 'water_any': True}\n",
            "Image 5: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1800x1200 at 0x79C9C95C0340>\n",
            "Label 5: {'bridges_any': False, 'buildings_any': True, 'buildings_affected_or_greater': False, 'buildings_minor_or_greater': False, 'debris_any': False, 'flooding_any': False, 'flooding_structures': False, 'roads_any': True, 'roads_damage': False, 'trees_any': True, 'trees_damage': False, 'water_any': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the processor and model\n",
        "processor = AutoImageProcessor.from_pretrained(\"MITLL/LADI-v2-classifier-large\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"MITLL/LADI-v2-classifier-large\")"
      ],
      "metadata": {
        "id": "PBgDIXe8h-Dx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to train mode\n",
        "model.train()"
      ],
      "metadata": {
        "id": "sgOYUo2niNhx",
        "outputId": "80f8dc6a-a478-4d4d-c730-7267974d870b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Swinv2ForImageClassification(\n",
              "  (swinv2): Swinv2Model(\n",
              "    (embeddings): Swinv2Embeddings(\n",
              "      (patch_embeddings): Swinv2PatchEmbeddings(\n",
              "        (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
              "      )\n",
              "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): Swinv2Encoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): Swinv2Stage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Swinv2Layer(\n",
              "              (attention): Swinv2Attention(\n",
              "                (self): Swinv2SelfAttention(\n",
              "                  (continuous_position_bias_mlp): Sequential(\n",
              "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                    (1): ReLU(inplace=True)\n",
              "                    (2): Linear(in_features=512, out_features=6, bias=False)\n",
              "                  )\n",
              "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (key): Linear(in_features=192, out_features=192, bias=False)\n",
              "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): Swinv2SelfOutput(\n",
              "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (drop_path): Swinv2DropPath(p=0.1)\n",
              "              (intermediate): Swinv2Intermediate(\n",
              "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): Swinv2Output(\n",
              "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (downsample): Swinv2PatchMerging(\n",
              "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Swinv2Stage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Swinv2Layer(\n",
              "              (attention): Swinv2Attention(\n",
              "                (self): Swinv2SelfAttention(\n",
              "                  (continuous_position_bias_mlp): Sequential(\n",
              "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                    (1): ReLU(inplace=True)\n",
              "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
              "                  )\n",
              "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
              "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): Swinv2SelfOutput(\n",
              "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "              (drop_path): Swinv2DropPath(p=0.1)\n",
              "              (intermediate): Swinv2Intermediate(\n",
              "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): Swinv2Output(\n",
              "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (downsample): Swinv2PatchMerging(\n",
              "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
              "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (2): Swinv2Stage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-17): 18 x Swinv2Layer(\n",
              "              (attention): Swinv2Attention(\n",
              "                (self): Swinv2SelfAttention(\n",
              "                  (continuous_position_bias_mlp): Sequential(\n",
              "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                    (1): ReLU(inplace=True)\n",
              "                    (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "                  )\n",
              "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): Swinv2SelfOutput(\n",
              "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (drop_path): Swinv2DropPath(p=0.1)\n",
              "              (intermediate): Swinv2Intermediate(\n",
              "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): Swinv2Output(\n",
              "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (downsample): Swinv2PatchMerging(\n",
              "            (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
              "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (3): Swinv2Stage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Swinv2Layer(\n",
              "              (attention): Swinv2Attention(\n",
              "                (self): Swinv2SelfAttention(\n",
              "                  (continuous_position_bias_mlp): Sequential(\n",
              "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                    (1): ReLU(inplace=True)\n",
              "                    (2): Linear(in_features=512, out_features=48, bias=False)\n",
              "                  )\n",
              "                  (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "                  (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "                  (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): Swinv2SelfOutput(\n",
              "                  (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "              (drop_path): Swinv2DropPath(p=0.1)\n",
              "              (intermediate): Swinv2Intermediate(\n",
              "                (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): Swinv2Output(\n",
              "                (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
              "  )\n",
              "  (classifier): Linear(in_features=1536, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "sENeoPjDiQyC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label keys\n",
        "label_keys = ['bridges_any', 'buildings_any', 'buildings_affected_or_greater', 'buildings_minor_or_greater',\n",
        "              'debris_any', 'flooding_any', 'flooding_structures', 'roads_any', 'roads_damage',\n",
        "              'trees_any', 'trees_damage', 'water_any']"
      ],
      "metadata": {
        "id": "-MNiwdDIiWLO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function for multilabel classification\n",
        "loss_func = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "FjVEdJUxicBz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "avqbjpzvimo5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(model, dataset, split_name, optimizer=None, train=False):\n",
        "    model.train() if train else model.eval()\n",
        "\n",
        "    batch_images = []\n",
        "    batch_labels = []\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, example in enumerate(tqdm(dataset[split_name])):\n",
        "        image = example['image']\n",
        "        labels = [int(example[key]) for key in label_keys]  # Convert label dictionary to list of 0s and 1s\n",
        "\n",
        "        batch_images.append(image)\n",
        "        batch_labels.append(labels)\n",
        "\n",
        "        # When the batch is full, process it\n",
        "        if (i + 1) % batch_size == 0:\n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
        "\n",
        "            # Convert labels to float32 to match the loss function's expectation\n",
        "            labels_tensor = torch.tensor(batch_labels, dtype=torch.float32)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**inputs, labels=labels_tensor)\n",
        "            loss = outputs.loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if train:\n",
        "                # Backward pass and optimization for training\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Get predictions (sigmoid to convert logits to probabilities)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(batch_labels)\n",
        "\n",
        "            # Reset the batch\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "\n",
        "    # Process remaining images in the last batch\n",
        "    if batch_images:\n",
        "        inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
        "\n",
        "        # Convert labels to float32 to match the loss function's expectation\n",
        "        labels_tensor = torch.tensor(batch_labels, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs, labels=labels_tensor)\n",
        "        loss = outputs.loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if train:\n",
        "            # Backward pass and optimization for training\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Get predictions\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "        all_predictions.extend(predictions)\n",
        "        all_labels.extend(batch_labels)\n",
        "\n",
        "    # Convert to numpy arrays for evaluation\n",
        "    all_labels = torch.tensor(all_labels).numpy()\n",
        "    all_predictions = torch.tensor(all_predictions).numpy()\n",
        "\n",
        "    return running_loss / len(all_predictions), all_labels, all_predictions"
      ],
      "metadata": {
        "id": "H0XgJNgkiqKh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute evaluation metrics\n",
        "def compute_metrics(labels, predictions, threshold=0.5):\n",
        "    # Convert probabilities to binary predictions using threshold\n",
        "    predictions_bin = (predictions >= threshold).astype(int)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, f1, and ROC AUC\n",
        "    accuracy = accuracy_score(labels, predictions_bin)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions_bin, average='macro', zero_division=1)\n",
        "    roc_auc = roc_auc_score(labels, predictions, average='macro', multi_class='ovr')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc"
      ],
      "metadata": {
        "id": "T_6HoixtjRoM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss, train_labels, train_preds = process_dataset(model, ds, 'train', optimizer, train=True)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validate after each epoch\n",
        "    val_loss, val_labels, val_preds = process_dataset(model, ds, 'validation')\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Compute metrics for validation\n",
        "    compute_metrics(val_labels, val_preds)"
      ],
      "metadata": {
        "id": "JbFFNn-2jYFB",
        "outputId": "2e1ca731-0525-4cfb-b0c4-bdbc9bbb1528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb5ef73035974e058eef837ce8c1544c"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}